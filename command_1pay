<<<<<<< HEAD
TMV && TTS
=======
change for upload

sudo systemctl is-enabled td-agent.service


>>>>>>> c44f92dfa8109e5271fdf7f926d4fbd9ae286ebf
#GIT
git config --global user.email "itlabvn.net@gmail.com"
git config --global user.name "Itlabvn"
ducdt@ubuntu16:~/upload$ git clone git@github.com:itlabvn/upload.git
git config --global user.name "Itlabvn"
cp ~/download/command_1pay /home/ducdt/upload

root@ubuntu16:/home/ducdt/command# git config --global push.default matching
root@ubuntu16:/home/ducdt/command# git config --global push.default simple
ducdt@ubuntu16:~/upload$ git status
ducdt@ubuntu16:~/command$ git add command_1pay 
ducdt@ubuntu16:~/upload$ git commit -m 'command line for remember'
ducdt@ubuntu16:~/upload$ git push
ducdt@ubuntu16:~/upload$ git remote -v
ducdt@ubuntu16:~/upload$ ssh-add ~/.ssh/id_rsa
ducdt@ubuntu16:~/upload$ git push

https://logs.bizdev.truemoney.com.vn
ducdt
12345612

 ps -aux | grep "cat /var/wap/clickmultimedia/logs/access_log" | grep "sh" | awk '{ print $2 }' | xargs kill 9
 ps -efw | grep "cat /var/wap/clickmultimedia/logs/access_log" | grep -v grep | awk '{print $2}' | xargs kill 9

for i in $(ls); do mv $i $i.j2; done
for i in $(ls); do mv $i $(echo $i | awk -F '.' '{ print $1"."$2}'); done


for i in {1..500}; do echo "I have a small favor to ask. More people are reading the nixCraft. Many of you block advertising which is your right, and advertising revenues are not sufficient to cover my operating costs. So you can see why I need to ask for your help. The nixCraft, takes a lot of my time and hard work to produce. If everyone who reads nixCraft, who likes it, contributes to support it with donations $i"; done

root@20e5bbd3c8a8:/# for i in {1..100}; do echo "Number $i. I have a small favor to ask. More people are reading the nixCraft. Many of you block advertising which is your right, and advertising revenues are not sufficient to cover my operating costs. So you can see why I need to ask for your help. The nixCraft, takes a lot of my time and hard work to produce. If everyone who reads nixCraft, who likes it, contributes to support it with donations.I have a small favor to ask. More people are reading the nixCraft. Many of you block advertising which is your right, and advertising revenues are not sufficient to cover my operating costs. So you can see why I need to ask for your help. The nixCraft, takes a lot of my time and hard work to produce. If everyone who reads nixCraft, who likes it, contributes to support it with donations. Bash for loop examples to make command line tasks more efficient "; done

docker run -ti --name test --log-driver=fluentd --log-opt tag=docker.{{.Name}} registry.truemoney.com.vn/tmvn/tts:v1.3.4-1-g6b00c35 /bin/bash
docker run -it --name test7 --log-driver=gelf --log-opt gelf-address=udp://192.168.3.252:12201 ubuntu:latest /bin/bash

${record["metadata"]["timestamp"]}

AND NOT _exists_:@@timestamp 

curl -u influx:password -G 'http://localhost:8086/query?db=telegraf' --data-urlencode 'q=SELECT mean(usage_idle) FROM cpu'

#GNS3
sudo add-apt-repository ppa:gns3/ppa
sudo apt-get update
sudo apt-get install gns3-gui
/usr/bin/gns3server
127.0.0.1
3080 TCP

# /usr/bin/snmpwalk  -v 2c -c public localhost ipadd
# /usr/bin/snmpwalk  -v 2c -c public localhost mem
rdesktop 192.168.3.200 -r disk:share=/home/ducdt/share
# cat snmpd.conf
syslocation "My Location"
syscontact  "Dan Massey"
sysservices 76

rocommunity public 
rocommunity  mycommstring  ip address

disk /

210.245.26.66

apt-get update && apt-get install -y procps
# update-rc.d td-agent defaults
# ls -l /etc/rc* | grep td-agent
# update-rc.d -f td-agent remove
root@1c67ccba8042:/usr/share/graylog# /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:NewRatio=1 -XX:MaxMetaspaceSize=256m -server -XX:+ResizeTLAB -XX:+UseConcMarkSweepGC -XX:+CMSConcurrentMTEnabled -XX:+CMSClassUnloadingEnabled -XX:+UseParNewGC -XX:-OmitStackTraceInFastThrow -jar -Dlog4j.configurationFile=/usr/share/graylog/data/config/log4j2.xml -Djava.library.path=/usr/share/graylog/lib/sigar/ -Dgraylog2.installation_source=docker /usr/share/graylog/graylog.jar server -f /usr/share/graylog/data/config/graylog.conf

tls-server
    key server-key.pem
    cert server-crt.pem
    ca ca-crt.pem
    dh dh2048.pem
    remote-cert-eku "TLS Web Client Authentication"


influx -username 'influx' -password 'password'
> show databases
name: databases
name
----
telegraf
_internal

> use telegraf
Using database telegraf
> show measurements
name: measurements
> SHOW USERS

root@srvu16:/etc/kapacitor# kapacitor ?
kapacitor define cpu_alert -type stream -tick cpu_alert.tick -dbrp telegraf.autogen
kapacitor record batch -task batch_cpu_alert -past 20m
kapacitor record stream -task cpu_alert -duration 60s
kapacitor list tasks
$ kapacitor show cpu_alert
oot@srvu16:/etc/kapacitor# kapacitor disable mem_alert
root@srvu16:/etc/kapacitor# kapacitor list tasks
ID        Type      Status    Executing Databases and Retention Policies
cpu_alert stream    disabled  false     ["telegraf"."autogen"]
mem_alert stream    disabled  false     ["telegraf"."autogen"]


# cat /etc/*-release
dpkg-deb -i nxlog-ce_2.9.1716_ubuntu_1604_amd64.deb
dpkg -i nxlog-ce_2.9.1716_ubuntu_1604_amd64.deb
apt install -fy
# vim /etc/nxlog/nxlog.conf 
<Extension _gelf>
    #Module      xm_syslog
    Module      xm_gelf
</Extension>

<Input eventlog>
    Module      im_msvistalog
    Exec $Message = to_json();
</Input>

<Processor buffer>
    Module      pm_buffer
    # 1Gb disk buffer 1048576 kilo-bytes
    MaxSize    1048576
    Type    Disk
    Directory  /tmp/buffer
</Processor>

<Output out>
    Module      om_udp
    Host        172.17.0.4
    Port        12201
    OutputType  GELF
</Output>

vim /etc/graylog/collector-sidecar/collector_sidecar.yml
server_url: http://127.0.0.1:9000/api/
update_interval: 10
tls_skip_verify: false
send_status: true
list_log_files:
node_id: graylog-collector-sidecar
collector_id: file:/etc/graylog/collector-sidecar/collector-id
cache_path: /var/cache/graylog/collector-sidecar
log_path: /var/log/graylog/collector-sidecar
log_rotation_time: 86400
log_max_age: 604800
tags:
    - linux
    - apache
    - docker
backends:
    - name: nxlog
      enabled: true
      binary_path: /usr/bin/nxlog
      configuration_path: /etc/nxlog/nxlog.conf

I'm finding it hard to find documentation around this math when trying to figure out why my instances are consuming 2GB+ of RAM as opposed to just 256MB
<source>
  @type forward
</source>


<match docker.**>
    @type gelf
    host 172.17.0.4
    port 12201
    buffer_type file
    buffer_path /var/log/td-agent/buffer
    buffer_chunk_limit 2m
    buffer_queue_limit 128
</match>

============== 

 <source>
    @type forward
  </source>
  <match **>
    @type copy
    <store>
      @type "gelf"
      host "172.17.0.4"
      port 12201
      protocol "tcp"
      flush_interval 10s
      request_timeout 120s
      buffer_type "file"
      buffer_path /tmp/buffer
      utf-8 
      buffer_chunk_limit 1m
      buffer_queue_limit 2048
      <buffer>
        flush_mode interval
        retry_type exponential_backoff
        @type file
        path /tmp/buffer
        flush_interval 10s
        chunk_limit_size 1m
        queue_limit_length 2048
      </buffer>
    </store>
    <store>
      @type "stdout"
    </store>
  </match>



FROM ubuntu:16.04
MAINTAINER Bas Meijer <bas.meijer@me.com>
LABEL running="docker run -d -p 8080:80 ubuntu:16.04"

ADD ansible /tmp/ansible

RUN apt update -y && \
    apt install -y apache2 && \
    apt-add-repository ppa:ansible/ansible && \
    apt-get update && \
    apt-get install ansible && \
    cd /tmp/ansible && \
    ansible-playbook playbook.yml

EXPOSE 8080

playbook.yml
---
- hosts: webservers
  vars:
    http_port: 80
    max_clients: 200
  tasks:
    - name: install apache packages
      apt: name=apache2 state=latest
    - name: copy apache config file
    - template:
        src: /template/apache.j2
        dest: /etc/apache2/apache.cfg
  handlers:
    - name: startup apache service
    - service:
        name: apache2
        state: restart
====
vim hosts
[webservers]
web1 ansible_ssh_port=22
web2 ansible_ssh_port=22
====
vim ansible.cfg
[defaults]
hostfile = hosts
remote_tmp = /tmp
===========
ansible-playbook --syntax-check --list-tasks -i hosts playbook.yml
->If there are no errors, you will get a list of tasks which the playbook wil execute:
ansible-playbook --list-tasks playbook.yml
->To run Ansible in Dry-Run Mode (a.k.a Check Mode):
ansible-playbook playbook.yml --check
$ tree -L 3


config.vm.boot_timeout = 600
vagrant up --provision --debug &> debug_log
$ VAGRANT_LOG=info vagrant up
or
$ set VAGRANT_LOG=info
$ vagrant up
372  docker logs -f some-elasticsearch 
  373  docker rm -vf some-elasticsearch
  374  docker run --name some-elasticsearch -it elasticsearch:2 elasticsearch -Des.cluster.name="graylog"
372  docker ps
  373  docker logs some-mongo 
  374  docker logs some-elasticsearch 
  375  docker logs some-elasticsearch | view -
  376  docker ps
  377  docker inspect some-elasticsearch | view 
  378  :qa
  379  docker inspect some-elasticsearch | view -
  380  docker logs some-elasticsearch | view -
  381  ip a
  382  docker exec -it some-elasticsearch ifconfig
  383  docker exec -it some-elasticsearch cat /etc/hosts
  384  docker logs some-elasticsearch | view -
  385  docker inspect silly_wing | view -
  386  docker restart some-elasticsearch 
  387  docker logs -f some-elasticsearch 
  388  docker exec silly_wing hostname
  389  docker stop silly_wing 
  390  docker run --link some-mongo:mongo --link some-elasticsearch:elasticsearch -p 12221:12221/udp -p 9000:9000 -e GRAYLOG_WEB_ENDPOINT_URI="http://127.0.0.1:9000/api" -it graylog/graylog
  391  docker ps -a
  392  docker rm -vf laughing_wescoff
  393  docker run --link some-mongo:mongo --link some-elasticsearch:elasticsearch -p 12201:12201/udp -p 9000:9000 -e GRAYLOG_WEB_ENDPOINT_URI="http://127.0.0.1:9000/api" -it graylog/graylog
  394  history
 372  docker run --help
  373  docker run --help | grep -i udp
  374  docker run --help | view -

http://docs.ansible.com/ansible/latest/modules_by_category.html
ansible all -m ping
ansible web1 -m apt -a "name=ntp state=installed" --sudo
ansible web1 -m copy -a "src=/home/vagrant/files/ntp.conf dest=/etc/ntp.conf mode=644 owner=root group=root" --sudo
ansible web1 -m service -a "name=ntp state=restarted"
ansible all -m shell -a "uptime"
ansible lb -m shell -a "/sbin/reboot"
ansible -i hosts 10.0.10.20 -m ping
# ansible-playbook --flush-cache graylog_playbook.yml
ansible-playbook -i dev tmv.yml -tags=nginx -DvvvC
# ansible-playbook  -i dev fluentd_deploy.yml --tag=server -DvvvC  --check
- hosts: localhost
  connection: local
  vars:
    run_block: true

  tasks:
  - block:
    - name: say hello
      command: echo hello
      register: hello

    - name: do something with results from say hello task
      command: echo {{ item }}
      with_items: "{{ hello.stdout_lines }}"
    when: run_block


docker run --privileged  -ti -e "container=docker"  -v /sys/fs/cgroup:/sys/fs/cgroup  trinitronx/ansible-base:stable-centos7  /usr/sbin/init
docker run --privileged  -ti -e "container=docker"  -v /sys/fs/cgroup:/sys/fs/cgroup  centos:latest  /usr/sbin/init

I see this suggestion to run --privileged -e "container=docker" -v /sys/fs/cgroup:/sys/fs/cgroup then run /usr/sbin/init:

docker run --privileged --name centos7 -v /sys/fs/cgroup:/sys/fs/cgroup:ro -p 80:80 -d centos /usr/sbin/init
docker run --privileged -it --name your_container_name centos7 /sbin/init
==========================
  544  docker run --name mongo -d mongo:3
  545  docker run --name elasticsearch     -e "http.host=0.0.0.0" -e "xpack.security.enabled=false"     -d docker.elastic.co/elasticsearch/elasticsearch:5.5.1
  546  docker run --link mongo --link elasticsearch     -p 9000:9000 -p 12201:12201 -p 514:514     -e GRAYLOG_WEB_ENDPOINT_URI="http://127.0.0.1:9000/api"     -d graylog/graylog:2.3.0-1
  547  docker run --name elasticsearch     -e "http.host=0.0.0.0" -e "xpack.security.enabled=false"  -e ES_JAVA_OPTS="-Xms512m -Xmx512m"   -d docker.elastic.co/elasticsearch/elasticsearch:5.5.1
  548  docker rm -vf elasticsearch
  549  docker run --name elasticsearch     -e "http.host=0.0.0.0" -e "xpack.security.enabled=false"  -e ES_JAVA_OPTS="-Xms512m -Xmx512m"   -d docker.elastic.co/elasticsearch/elasticsearch:5.5.1
  550  docker ps
  551  docker logs elasticsearch -f
  552  docker ps
  553  docker rm -vf elasticsearch
sudo sysctl -w vm.max_map_count=262144
  554  docker run --name elasticsearch     -e "http.host=0.0.0.0" -e "xpack.security.enabled=false"  -e JAVA_OPTS="-Xms512m -Xmx512m"   -d docker.elastic.co/elasticsearch/elasticsearch:5.5.1
  555  docker logs elasticsearch -f
  556  docker rm -vf elasticsearch
  557  sudo sysctl -w vm.max_map_count=262144
  558  docker run --name elasticsearch     -e "http.host=0.0.0.0" -e "xpack.security.enabled=false" -p 9200:9200 -p 9300:9300 -d docker.elastic.co/elasticsearch/elasticsearch:5.5.1
  559  docker ps -a
  560  docker logs elasticsearch -f
  561  docker ps
  562  docker ps -a
  563  docker rm -vf zen_kirch
  564  docker run --link mongo --link elasticsearch   --name graylog  -p 9000:9000 -p 12201:12201/tcp -p 514:514  -p 22222:22222/udp -p 33333:33333/tcp -p 44444:44444/udp -p 24225:24225/tcp  -e GRAYLOG_WEB_ENDPOINT_URI="http://127.0.0.1:9000/api"     -d graylog/graylog:2.3.0-1
  565  docker ps -a
  566  docker logs angry_booth -f
echo -n '{ "version": "1.1", "host": "example.org", "short_message": "A short message", "level": 5, "_some_info": "foo" }' | nc -w0 -u 172.17.0.4 12201
echo -n '{ "version": "1.1", "host": "example.org", "short_message": "A short message", "level": 6, "_some_info": "foo" }' | nc -w 1 -u 172.17.0.4 12201
#GELF TCP
For sending GELF over TCP you need to delimit the message by a NUL byte.
This is a little awkward in the shell, but otherwise the GELF processor does not know where the message ends, since GELF has no length specified.
Try using:
echo -ne '{"version": "1.1","host":"example.org","short_message":"A short message that helps you identify what is going on","full_message":"Backtrace here\\n\\nmore stuff","level":1,"_user_id":9001,"_some_info":"foo","_some_env_var":"bar"}\x00' > /tmp/gelf.bin
cat /tmp/gelf.bin | netcat -w 1 localhost 514
Please note the doubly escape newlines, since echo is resolving one level of escapes with -e. The \x00 at the end is the delimiter and not part of the message that gets indexed.
A GELF library will do this for you, so this really only applies when sending this manually. I usually resort to either HTTP or UDP for convenience.
+ GELF via TCP requires messages to be terminated by a null character (\0). I agree that this is badly (or rather not at all) documented and we'll fix that.
echo -e "{\"version\": \"1.1\",\"host\":\"example.org\",\"short_message\":\"This is short message\",\"level\":1}\0" | nc -w 1 172.17.0.4 514

echo -e "{\"version\": \"1.1\",\"host\":\"example.org\",\"short_message\":\"This is short message\",\"level\":1}\0" | nc -w 1 127.0.0.1 24224

echo -e '{"message":"TEST MESSAGE","host":"hd1app1","service":"test_service"}\0' | nc 10.0.1.138 42185
curl -X POST -d 'json={"json":"message"}' http://localhost:8888/debug.test

/usr/sbin/td-agent-gem install gelf
https://raw.githubusercontent.com/emsearcy/fluent-plugin-gelf/master/lib/fluent/plugin/out_gelf.rb

docker run --link mongo --link elasticsearch   --name graylog  -p 9000:9000 -p 12201:12201 -p 514:514  -p 24225:24225/tcp  -e GRAYLOG_WEB_ENDPOINT_URI="http://127.0.0.1:9000/api"     -d graylog/graylog:2.3.0-1

ducdt@ubuntu16:~$ curl localhost:9200/graylog_0/_search?pretty 
curl -XDELETE 'localhost:9200/graylog_0'
curl 'localhost:9200/_cat/indices?v'

# gem search -rd fluent-plugin
# gem install fluent-plugin-gelf-hs
# apt-get install ruby-all-dev
td-agent --dry-run
ps aux | grep ruby
root@ubuntu16:/etc/td-agent/plugin# ps -aux | grep ruby
td-agent 22705  0.0  0.3 128812 31176 ?        Sl   16:47   0:00 /opt/td-agent/embedded/bin/ruby /usr/sbin/td-agent --log /var/log/td-agent/td-agent.log --daemon /var/run/td-agent/td-agent.pid
td-agent 22820  0.2  0.5 150436 45252 ?        Sl   16:50   0:00 /opt/td-agent/embedded/bin/ruby /usr/sbin/td-agent --log /var/log/td-agent/td-agent.log --daemon /var/run/td-agent/td-agent.pid
root     22835  0.0  0.0  22820  1024 pts/21   S+   16:50   0:00 grep --color=auto ruby
root@ubuntu16:/etc/td-agent/plugin# kill -9 22705 22820
===================

docker run -ti --name test --log-driver=fluentd --log-opt tag=docker.{{.Name}} registry.truemoney.com.vn/tmvn/tts /bin/bash

root@ubuntu16:/etc/td-agent/plugin# /usr/sbin/td-agent-gem env
root@ubuntu16:/etc/td-agent# fluent-gem env
# /usr/sbin/td-agent-gem search -rd fluent-plugin | grep gelf
root@ubuntu16:/etc/td-agent/plugin# /usr/sbin/td-agent-gem install fluent-plugin-input-gelf
/var/lib/gems/2.3.0/gems/fluentd-0.14.21/lib/fluent/plugin

docker run -ti --name test --log-driver=fluentd --log-opt tag=docker ubuntu /bin/bash
# docker run -td --name test --log-driver=fluentd --log-opt 	 ubuntu

ansible-playbook -i dev -e version=v1.3.4-1-g6b00c35 tts-deploy.yml -Dvvv

# ansible-playbook -i dev tts-deploy.yml -e tts.version=v1.3.4-1-g6b00c35 -Dvvv
# ansible-playbook -i dev tmv-deploy.yml -e tts.version=v1.3.2-1-gc852a17 -Dvvv
treo: enter, enter ~. =>(thoat)
sudo -iu deploy
docker stats $(docker ps --format={{.Names}})

for i in {1..10}; do echo "{\"a\": $i }"; done >> /home/some.log

-rw-rw---- 1 mysql mysql  14K Th11 15 06:48 /data/mysql/xmp/tbl_msgtransact_ok.frm
-rw-rw---- 1 mysql mysql  12G Th11 15 13:10 /data/mysql/xmp/tbl_msgtransact_ok.ibd

ThClick@567


docker run -ti --name test --log-driver=fluentd --log-opt fluentd-buffer-limit=10KB  --log-opt tag=docker.{{.Name}} ubuntu /bin/bash



2 Create a one-week retention policy
CREATE RETENTION POLICY "one_week" ON "telegraf" DURATION 1w REPLICATION 1 DEFAULT
- CREATE RETENTION POLICY "one_hour" ON "telegraf" DURATION 1h REPLICATION 1 DEFAULT
SHOW RETENTION POLICIES ON "telegraf"
3 Create a four-week retention policy
CREATE RETENTION POLICY "four_week" ON "telegraf" DURATION 4w REPLICATION 1
4. Create a Continuous Query (CQ) 
- Fully-qualify a measurement: "<database>"."<retention_policy>"."<measurement>"
CREATE CONTINUOUS QUERY "ave_usage" ON "telegraf" 
BEGIN 
	SELECT MEAN("usage_idel")
	INTO "telegraf"."four_week"."ave_cpu"
	FROM "telegraf"."one_week"."cpu"
	WHERE "cpu" = "cpu-total" GROUP BY time(5m)
END
5.Test (Wait about 5 minues)
USE "telegraf"
SHOW MEASUREMENT
SELECT "usage_idle" FROM "telegraf"."one_week"."cpu" LIMIT 3
SELECT "mean" FROM "telegraf"."four_week"."ave_cpu" LIMIT 3
6. Issue-Working with historical data
>SELECT MAX("water_level") INTO "maximums" FROM "h2o_feet" 
  WHERE time >= '2015-08-18T00:00:00Z' AND time <= '2015-08-18T00:18:00Z' GROUP BY time(12m)
>SELECT * FROM "maximums"; (2 rows return)
>SELECT "water_level","location" FROM "h2o_feet" LIMIT 8;
=============
>precision rfc3339  #convert time reable
>SELECT * FROM "french_bulldogs"> 
---------------
> show databases
> show retention policies on telegraf
name    duration shardGroupDuration replicaN default
----    -------- ------------------ -------- -------
autogen 720h0m0s 168h0m0s           1        true
forever 0s       168h0m0s           1        false
> drop retention policy forever on telegraf
> CREATE RETENTION POLICY "one_hours" ON "telegraf" DURATION 1h REPLICATION 1 DEFAULT
> CREATE RETENTION POLICY "a_year" ON "telegraf" DURATION 52w  REPLICATION 1;
> CREATE CONTINUOUS QUERY "a_year" ON "telegraf" BEGIN SELECT mean(value) AS value INTO telegraf."a_year".:MEASUREMENT FROM /.*/ GROUP BY time(5m), * END
> SHOW CONTINUOUS QUERIES
> DROP CONTINUOUS QUERY a_year ON telegraf;
> select "usage_idle" from "cpu" group by * order by desc limit 1

> create database downsampled_telegraf;
> CREATE RETENTION POLICY "a_year" ON "telegraf" DURATION 52w  REPLICATION 1;


timeout 5 md5sum /dev/zero

> create database downsampled_telegraf;
> CREATE RETENTION POLICY "four_week" ON "downsampled_telegraf" DURATION 4w  REPLICATION 1;
ALTER RETENTION POLICY "autogen" ON "downsampled_telegraf" DURATION 52w REPLICATION 1 SHARD DURATION 10w DEFAULT
> SELECT mean(*) INTO "downsampled_telegraf"."four_week".:MEASUREMENT FROM telegraf."autogen"./.*/ WHERE time >= '2015-08-18T00:00:00Z' AND time <= '2015-08-18T00:18:00Z' GROUP BY time(5m)
> SELECT mean(*) INTO "downsampled_telegraf"."four_week".:MEASUREMENT FROM telegraf."autogen"./.*/ WHERE time <= now() GROUP BY time(5m) 
> ALTER RETENTION POLICY "autogen" ON "telegraf" DURATION 2h REPLICATION 1 SHARD DURATION 1h DEFAULT
> CREATE CONTINUOUS QUERY "cq_four_week" ON "telegraf" BEGIN SELECT mean(*) INTO "downsampled_telegraf"."four_week".:MEASUREMENT FROM telegraf."autogen"./.*/ GROUP BY time(5m),* END

SELECT mean(*) INTO "downsampled_telegraf"."autogen".:MEASUREMENT FROM telegraf."autogen"./.*/ WHERE time <= now() GROUP BY time(5m),host
SELECT mean(*) INTO "downsampled_telegraf"."autogen".:MEASUREMENT FROM telegraf."autogen"./.*/ WHERE time <= now() GROUP BY location,time(5m) fill(none)
> CREATE RETENTION POLICY "one_hour" ON "telegraf" DURATION 1h REPLICATION 1 DEFAULT

> influx -precision rfc3339
> CREATE DATABASE downsampled_telegraf;
> CREATE DATABASE downsampled2_telegraf;
> ALTER RETENTION POLICY "autogen" ON "downsampled_telegraf" DURATION 3h REPLICATION 1 SHARD DURATION 1h DEFAULT
> SELECT mean(*) INTO "downsampled_telegraf"."autogen".:MEASUREMENT FROM telegraf."autogen"./.*/ WHERE time <= now() GROUP BY time(5m),*
> SELECT mean(*) INTO "downsampled2_telegraf"."autogen".:MEASUREMENT FROM telegraf."autogen"./.*/ WHERE time <= now() GROUP BY time(10m),*
> ALTER RETENTION POLICY "autogen" ON "telegraf" DURATION 1h REPLICATION 1 SHARD DURATION 1h DEFAULT
> CREATE CONTINUOUS QUERY "cq_5m" ON "telegraf" BEGIN SELECT mean(*) INTO "downsampled_telegraf"."autogen".:MEASUREMENT FROM telegraf."autogen"./.*/ GROUP BY time(5m),* END
> CREATE CONTINUOUS QUERY "cq_10m" ON "telegraf" BEGIN SELECT mean(*) INTO "downsampled2_telegraf"."autogen".:MEASUREMENT FROM telegraf."autogen"./.*/ GROUP BY time(10m),* END

> SELECT * FROM downsampled_telegraf."autogen".cpu ORDER BY time DESC LIMIT 20;
> SELECT * FROM telegraf."autogen".cpu ORDER BY time DESC LIMIT 20;
> SHOW CONTINUOUS QUERIES
> SHOW RETENTION POLICIES ON "telegraf"
> DROP CONTINUOUS QUERY a_year ON telegraf;
> ALTER RETENTION POLICY "autogen" ON "telegraf" DURATION 2h REPLICATION 1 SHARD DURATION 1h DEFAULT

alter retention policy "autogen" on "telegraf" duration INF replication 1;
alter retention policy "autogen" on "telegraf" duration INF replication 1 shard duration 7d;

>precision rfc3339  #convert time reable
$ influx -precision rfc3339
> show shards
> SHOW MEASUREMENTS where host='serverA'
> SHOW TAG KEYS 
> SHOW TAG KEYS from CPU
> select * from telegraf."autogen".cpu order by time desc limit 5;
> select * from downsampled_telegraf."four_week".cpu order by time desc limit 5;
> select * from telegraf."one_hour".cpu;
> select * from telegraf.autogen.system where ip='172.31.1.17' order by time desc limit 5;

SELECT mean("bytes_sent") AS "bytes_send" FROM "net" WHERE ("host" = 'srvu16') AND $timeFilter GROUP BY time($__interval) fill(null)
SELECT mean("bytes_sent") AS "bytes_send" FROM "four_week"."cpu" WHERE ("host" = 'srvu16') AND $timeFilter GROUP BY location,time(5m) fill(none)
SELECT mean(*) INTO "downsampled_telegraf"."autogen".:MEASUREMENT FROM telegraf."autogen"./.*/ WHERE time <= now() GROUP BY time(5m),*


SHOW MEASUREMENTS where host='ip-172-31-1-17'
precision rfc3339
select * from telegraf.autogen.system where ip='172.31.1.17' order by time desc limit 5;
select cpu, usage_idle from telegraf.autogen.cpu where ip='172.31.1.17' and cpu='cpu-total' order by time desc limit 5;
select result_type from telegraf.autogen.net_response  order by time desc limit 5;
select * from telegraf.autogen.net_response  order by time desc limit 1;  //check_tcp.tick
kapacitor define check_tcp -type stream -tick /etc/kapacitor/ticks/stream/check_tcp.tick -dbrp telegraf.autogen
# kapacitor record stream -task check_tcp -duration 1m
# kapacitor list recordings $rid

influx -host '172.31.3.11' -port '8086' -username 'telegraf' -password 'telegraf'
kapacitor replay-live batch -task real_pred_task -rec-time -past 200d

push.push_buffer reach

ALTER TABLE push_buffer MODIFY id INT;
mysqldump -u <db_username> -h <db_host> -p db_name table_name > table_name.sql
mysqldump -usuwat -p5bOO74bN1d -h 10.20.0.3 push push_buffer > push_push_buffer.sql

vn-wallet thì hỏi phuongnh@truemoney.com.vn và hoangnn@truemoney.com.vn
anv2 thì có anhtt@truemoney.com.vn
bankgw + billpay thì binhhc@truemoney.com.vn
anv1 thì em cũng ko rõ là ai, a có thể hỏi anhtv@truemoney.com.vn

anhtv@truemoney.com.vn


cpu
disk
diskio
kernel
kernel_vmstat
mem
net
netstat
processes
swap
system

mysql -usuwat -p5bOO74bN1d -h 10.20.0.3
issue is : table push.push_buffer reach max id
<source>
  @type tcp
  port 24224
  bind 0.0.0.0
</source>



#<match **>
#  type stdout
#</match>

<match **>
  @type copy
  <store>
    @type gelf
    host 172.17.0.4
    port 22222
    buffer_type file
    buffer_path /var/log/td-agent/buffer
    flush_interval 10s
  </store>
  <store>
   type stdout
  </store>
</match>

===============

<match **>
  @type copy
  <store>
    type gelf
    host 172.17.0.4
#    port 12201
    port 24225
    protocol tcp
    buffer_type file
    buffer_path /var/log/td-agent/buffer/td
    flush_interval 10s
  </store>
</match>

# Log Forwarding
<match **>
  type forward

  <server>
    host 172.17.0.4
    port 24225
  </server>


  # use tcp for heartbeat
  heartbeat_type tcp

  # use longer flush_interval to reduce CPU usage.
  # note that this is a trade-off against latency.
  flush_interval 10s

  # use multi-threading to send buffered data in parallel
  num_threads 8

  # expire DNS cache (required for cloud environment such as EC2)
  expire_dns_cache 600

  # use file buffer to buffer events on disks.
  buffer_type file
  buffer_path /var/log/td-agent/buffer/forward

  # in case buffer becomes full, have local backup
  <secondary>
    type file
    path /var/log/td-agent/secondary
    compress gzip
  </secondary>
</match>

=========================
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

#<match **>
#  @type file
#  path /var/log/td-agent/mylog
#</match>

<match **>
  @type cope
  <store>
    type gelf
    host 172.17.0.4
    port 22222
    buffer_type file
    buffer_path /var/log/td-agent/buffer
    flush_at_shutdown true
    flush_interval 10s
  </store>
  <store>
    type file
    path /var/log/td-agent/myapp
  </store>
</match>


===================
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

#<match **>
#  @type file
#  path /var/log/td-agent/mylog
#</match>

<match **>
  @type copy
  <store>
    type gelf
    host 172.17.0.5
    port 12201
    flush_interval 5s
  </store>
</match>

==================
<source>
  type http
  port 8888
</source>

<source>
  type forward
#  port 24224
#  bind 0.0.0.0
</source>

<match **>
  type file
  path /var/log/td-agent/mylog
</match>

#Graylog
<match **>
  type forward
  send_timeout 60s
  recover_wait 10s
  heartbeat_interval 1s
  phi_threshold 8
  hard_timeout 60s

  <server>
    name graylogserver
    host 172.17.0.5
    port 12201
    #weight 60
  </server>

  <secondary>
    type file
    path /var/log/td-agent/forward-failed
  </secondary>
</match>


# Include config files in the ./config.d directory
#@include config.d/*.conf
=================

<match **>
  type forward
  <server>
    host 172.17.0.5
    port 12201
  </server>
  buffer_type file
  buffer_path /var/log/td-agent/buffer/td.*.buffer
  buffer_chunk_limit 128m
  buffer_queue_limit 64
  flush_interval 20s
</match>



# cat /etc/lsb-release 
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=16.04
DISTRIB_CODENAME=xenial
DISTRIB_DESCRIPTION="Ubuntu 16.04.3 LTS"


# docker info | grep 'Logging Driver'
WARNING: No swap limit support
Logging Driver: json-file
# docker run --log-driver=gelf --log-opt gelf-address=udp://172.17.0.4:12201 -p 80:80 -it --name container2 -d ubuntu:latest /bin/bash
vim /etc/docker/daemon.json
{
  "log-driver": "gelf"
}
docker inspect -f '{{.HostConfig.LogConfig.Type}}' container2
$ docker inspect mongo | grep LogPath
$ tail -n2 LogPath
docker run --log-driver=syslog --log-opt tag="nginx" --log-opt syslog-address=udp://[IP]:1514 nginx
===============
544  docker run --name mongo2 -d mongo:3
docker run --name some-elasticsearch -d elasticsearch:2 elasticsearch -Des.cluster.name="graylog"
676  docker run --name elasticsearch2     -e "http.host=0.0.0.0" -e "xpack.security.enabled=false"  -e ES_JAVA_OPTS="-Xms512m -Xmx512m"   -d docker.elastic.co/elasticsearch/elasticsearch:5.5.1
675  docker run --link mongo2 --link elasticsearch2     -p 9000:9000 -p 12201:12201 -p 514:514     -e GRAYLOG_WEB_ENDPOINT_URI="http://127.0.0.1:9000/api"     -d graylog/graylog:2.3.0-1

docker run --log-driver=json-file --name "vidu" -d docker.elastic.co/elasticsearch/elasticsearch:5.5.1
docker run --log-driver=gelf --log-opt gelf-address=udp://172.17.0.4:514 ubuntu:latest echo hello world

vim /etc/apt/sources.list 
deb http://br.archive.ubuntu.com/ubuntu/ xenial main restricted
deb-src http://br.archive.ubuntu.com/ubuntu/ xenial main restricted

deb http://br.archive.ubuntu.com/ubuntu/ xenial-updates main restricted
deb-src http://br.archive.ubuntu.com/ubuntu/ xenial-updates main restricted

deb http://br.archive.ubuntu.com/ubuntu/ xenial universe
deb-src http://br.archive.ubuntu.com/ubuntu/ xenial universe
deb http://br.archive.ubuntu.com/ubuntu/ xenial-updates universe
deb-src http://br.archive.ubuntu.com/ubuntu/ xenial-updates universe

deb http://br.archive.ubuntu.com/ubuntu/ xenial multiverse
deb-src http://br.archive.ubuntu.com/ubuntu/ xenial multiverse
deb http://br.archive.ubuntu.com/ubuntu/ xenial-updates multiverse
deb-src http://br.archive.ubuntu.com/ubuntu/ xenial-updates multiverse

deb http://br.archive.ubuntu.com/ubuntu/ xenial-backports main restricted universe multiverse
deb-src http://br.archive.ubuntu.com/ubuntu/ xenial-backports main restricted universe multiverse

deb http://security.ubuntu.com/ubuntu xenial-security main restricted
deb-src http://security.ubuntu.com/ubuntu xenial-security main restricted
deb http://security.ubuntu.com/ubuntu xenial-security universe
deb-src http://security.ubuntu.com/ubuntu xenial-security universe
deb http://security.ubuntu.com/ubuntu xenial-security multiverse
deb-src http://security.ubuntu.com/ubuntu xenial-security multiverse

deb http://archive.canonical.com/ubuntu xenial partner
deb-src http://archive.canonical.com/ubuntu xenial partner
=======================
$ sudo rm /etc/apt/sources.list.d/*
$ sudo apt-get update && sudo apt-get dist-upgrade

sed -i "s/$(sed -ne '/transapi/,/^$/p' dev | grep version | awk '{ print $2 }')/123456/g" dev

# git add -p
# git status
# git diff
# git commit -m "Request second time for deploy fluentd playbook"
# git push -u
# git checkout ducd  //comebackup branch ducdt

sudo docker run -ti --name test --log-driver=fluentd --log-opt tag=docker fc9cb27a571b /bin/bash

docker run -it --name test --log-driver gelf –-log-opt gelf-address=udp://192.168.3.252:12201 ubuntu /bin/bash
docker run --log-driver gelf –-log-opt gelf-address=udp://192.168.3.252:12201 ubuntu echo hello world
docker run -ti --name  --log-driver gelf –-log-opt gelf-address=udp://192.168.3.252:12201 ubuntu:latest /bin/bash

curl 0:80

root@ubuntu16:~/1PAY/tmvn_ansible# git status
root@ubuntu16:~/1PAY/tmvn_ansible# git branch
* ducdt
  master
root@ubuntu16:~/1PAY/tmvn_ansible# git checkout -b ducdt #create ducdt branch
git fetch
git merge FETCH_HEAD
git branch

2039  git add -p
      git add . (them tat ca thay doi)
 2040  git status
git add files/kapacitor/tasks.sh

 2041  git commit -m "Update fluentd_deploy.yml playbook and config"
 2042  git push -u# # 
 2043  git checkout master
 2044  git fetch
 2045  git merge FETCH_HEAD
 2046  git checkout -b deploy_fluent_dev
#add more to ready not commit
git add -p
git add templates/fluentd/gelf-3.0.0.gem
git commit --amend
git push -fu


for i in $(find /etc/kapacitor/ticks -name \*.tick -print); do echo //hello >> $i; done

source:ip\-172\-31\-1\-237 AND container_name:\/tts
ansible-playbook -i dev tts-deploy.yml -e "tts.version=v1.3.4-1-g6b00c35" -DvvvC
# ansible-playbook -i dev --extra-vars "version=v1.3.4-1-g6b00c35" tts-deploy.yml -DvvvC

#ticks_changed: "{{ batch_item.results | selectattr('changed') | map(attribute='item') | list }} + {{ stream_item.results | selectattr('changed') | map(attribute='item') | list }} + {{ temp_item.results | selectattr('changed') | map(attribute='item') | list }}"

[[inputs.ping]]
   urls = ["172.31.0.54", "172.31.0.55", "172.31.1.13", "172.31.1.14", "172.31.1.15", "172.31.1.237", "172.31.1.41", "172.31.2.4", "172.31.2.5", "172.31.3.11", ]
   count = 10

$ influx -host '172.31.3.11' -port '8086' -username 'telegraf' -password 'telegraf'
#kapacitor define port_response -type stream -tick port_response.tick -dbrp telegraf.autogen
# kapacitor list recordings $rid

Open "dconf-editor"

"dconf editor" is available for installation in the ubuntu software center

navigate to org > gnome > gnome-screenshot and change the default-file-type png to jpg.

telegraf --config /etc/telegraf/telegraf.conf -test
etrieved_value / (100 (seconds) * 60 (minutes) * 60 (hours) * 24 (days))          

root@2e9809a14469:/opt/true-money/www/truetts# cat /etc/supervisor/supervisord.conf
; supervisor config file

[unix_http_server]
file=/var/run/supervisor.sock   ; (the path to the socket file)
chmod=0700                       ; sockef file mode (default 0700)

[supervisord]
logfile=/var/log/supervisor/supervisord.log ; (main log file;default $CWD/supervisord.log)
pidfile=/var/run/supervisord.pid ; (supervisord pidfile;default supervisord.pid)
childlogdir=/var/log/supervisor            ; ('AUTO' child log dir, default $TEMP)

; the below section must remain in the config file for RPC
; (supervisorctl/web interface) to work, additional interfaces may be
; added by defining them in separate rpcinterface: sections
[rpcinterface:supervisor]
supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface

[supervisorctl]
serverurl=unix:///var/run/supervisor.sock ; use a unix:// URL  for a unix socket

; The [include] section can just contain the "files" setting.  This
; setting can list multiple files (separated by whitespace or
; newlines).  It can also contain wildcards.  The filenames are
; interpreted as relative to this file.  Included files *cannot*
; include files themselves.

[include]
files = /etc/supervisor/conf.d/*.conf

=================
root@2e9809a14469:/etc/supervisor/conf.d# cat programs.conf 
[program:nginx]
command=nginx -g 'daemon off;'
stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
stderr_logfile=/dev/stderr
stderr_logfile_maxbytes=0

[program:php-fpm]
command=php-fpm
stdout_logfile=/dev/stdout
stdout_logfile_maxbytes=0
stderr_logfile=/dev/stderr
stderr_logfile_maxbytes=0

=======================
root@ip-172-31-2-5:/home/ubuntu# curl 'http://172.31.2.5:9200/_cat/indices/graylog*?v'
health status index      pri rep docs.count docs.deleted store.size pri.store.size 
green  open   graylog_14   4   0      52360            0     36.2mb         36.2mb 
green  open   graylog_25   4   0     134782            0    115.6mb        115.6mb 
green  open   graylog_13   4   0      86803            0     91.5mb         91.5mb 
green  open   graylog_24   4   0       8540            0      4.2mb          4.2mb 
green  open   graylog_16   4   0      48304            0     36.7mb         36.7mb 
green  open   graylog_27   4   0      83679            0     89.3mb         89.3mb 
green  open   graylog_15   4   0      75826            0     46.4mb         46.4mb 
green  open   graylog_26   4   0      82467            0     78.4mb         78.4mb 
green  open   graylog_18   4   0      63516            0       49mb           49mb 
green  open   graylog_29   4   0       2391            0      3.3mb          3.3mb 
green  open   graylog_17   4   0      31106            0     16.7mb         16.7mb 
green  open   graylog_28   4   0     152308            0    166.4mb        166.4mb 
green  open   graylog_19   4   0      53479            0     39.6mb         39.6mb 
green  open   graylog_10   4   0       9950            0     10.6mb         10.6mb 
green  open   graylog_21   4   0      52221            0     51.6mb         51.6mb 
green  open   graylog_20   4   0      65989            0     45.4mb         45.4mb 
green  open   graylog_12   4   0      66880            0     62.3mb         62.3mb 
green  open   graylog_23   4   0       8540            0      4.2mb          4.2mb 
green  open   graylog_11   4   0      55403            0       33mb           33mb 
green  open   graylog_22   4   0      61365            0     53.8mb         53.8mb 







wallet-proxy

logs.dev.truemoney.com.vn
ducdt
54678912 (edited)




Lời đầu tiên xin chúc mừng Anh/Chị   đã trở  thành thành viên chính thức tại đại gia đình 1Pay. Thay mặt tập thể 1Pay xin chúc Anh/Chị nhiều sức khỏe và cùng với 1Pay đồng hành trên mọi nẻo đường. Sau đây em xin gửi lại Anh/Chị ‘Thông tin tài khoản Email & acount MOG_daily’. Anh/Chị đăng nhập và đổi lại pass theo thông tin dưới đây:

1. Email:

Username: ducdoan@truemoney.com.vn

Password: 12345678

Chữ ký: Anh/Chị làm theo mẫu phía dưới ở phần chữ ký.

2. MOG_daily: là trang giao tiếp nội bộ của MOGer, mọi hoạt động về trao đổi công việc & thông tin update của Công ty sẽ post lên trang này:
  ducdoan@truemoney.com.vn

Login theo hướng dẫn trong mail công ty:  ducdoan@truemoney.com.vn

3. Các quy định nội bộ: Anh/Chị login vào email:   ducdoan@truemoney.com.vn để xem tài liệu nhé.

Mọi thắc mắc Anh/chị vui lòng liên hệ Ms. Ly – Chuyên viên Nhân Sự (Skype: Thaoly.hust)
<source>
  @type forward
  port 24224
</source>

<source>
  @type forward
  port 9012
</source>

<filter docker.**>
  @type record_transformer
  enable_ruby
  <record>
     container_name ${record["container_name"].gsub('/','')}
     host ${record["source"]}
     timestamps ${record["timestamps"]}
     #formatted_time ${time.strftime('%Y-%m-%dT%H:%M:%S%4N')}

#     timestamp ${record["times"].strftime('%Y-%m-%d %H:%M:%S.%N')}
#     format_time ${Time.now.strftime('%Y-%m-%dT%H:%M:%S.%4N')}
     #format_time ${Time.now.strftime('%Y-%m-%dT%H:%M:%S.%N%z')}
#     f_time ${Time.now.iso8601(3)}
    # formatted_time ${time.strftime('%Y-%m-%dT%H:%M:%S%z')}
     #timestamp ${ require 'time'; Time.now.utc.iso8601(3) }
     #time_nano ${t = Time.now; ((t.to_i * 1000000000) + t.nsec).to_s.rsrip}
 #    time_key @timestamp
 #    time_format "%Y-%m-%dT%H:%M:%S.%L%Z"
  </record>
</filter>

<match docker.**>
  @type gelf
  host 127.0.0.1
  port 12201
  flush_interval 1s
  use_record_host true
</match>


172.32.0.11
172.32.0.12
172.32.1.11
172.32.1.12
172.32.1.14
172.32.1.15
172.32.1.16
172.32.1.17
172.32.1.18
172.32.1.19

2017-10-31 14:22:29 +0700 [warn]: #0 fluent/root_agent.rb:299:emit_error_event: dump an error event: error_class=RuntimeError error="failed to expand `t = Time.now; ((t.to_i * 1000000000) + t.nsec).to_s.strftime(\"%Y-%m-%dT%H:%M:%S.%L\")` : error = undefined method `strftime' for \"1509434549316059243\":String\nDid you mean?  strip" tag="docker" time=1509434546 record={"source"=>"ubuntu16", "log"=>"nice 18\r", "container_id"=>"5c09e0f66afbf50a64166e18ba4552590ddfbadb5075de0a0da88e586c38827a", "container_name"=>"/test"}

docker run --log-driver gelf –-log-opt gelf-address=udp://192.168.3.252:12201 alpine echo hello world


<source>
  @type forward
  port 9012
</source>

<source>
  @type forward
  port 24224
</source>

<filter docker.**>
  @type record_transformer
  enable_ruby
  <record>
     container_name ${record["container_name"].gsub('/','')}
     host ${record["source"]}
  </record>
</filter>

<match docker.**>
  @type gelf
  host 127.0.0.1
  port 9010
  buffer_type file
  buffer_path /var/log/td-agent/buffer/docker.*.buffer
  flush_interval 1s
  use_record_host true
</match>
====================================
<source>
  @type forward
  port 24224
</source>

<filter docker.**>
  @type record_transformer
  enable_ruby
  <record>
     source "#{Socket.gethostname}"
  </record>
</filter>

<match docker.**>
  @type forward
  buffer_type file
  buffer_path /var/log/td-agent/buffer/docker.*.buffer
  flush_interval 3s
  <server>
    host 172.31.1.237
    port 9012
  </server>
</match>

172.32.1.11
172.32.1.12
172.32.1.14
172.32.1.15
172.32.1.16
172.32.1.17
172.32.1.18
172.32.1.19

[httpd] 127.0.0.1 - - [08/Nov/2017:04:16:38 +0000] "GET /kapacitor/v1/tasks?dot-view=attributes&fields=link&limit=100&offset=0&pattern=load&replay-id=&script-format=formatted HTTP/1.1" 200 43 "-" "KapacitorClient" 9dd1fa9f-c43b-11e7-83de-000000000000 443
[httpd] 127.0.0.1 - - [08/Nov/2017:04:16:38 +0000] "GET /kapacitor/v1/tasks/load?dot-view=attributes&replay-id=&script-format=formatted HTTP/1.1" 404 57 "-" "KapacitorClient" 9dd274c0-c43b-11e7-83df-000000000000 341
[httpd] 127.0.0.1 - - [08/Nov/2017:04:16:38 +0000] "POST /kapacitor/v1/tasks HTTP/1.1" 200 1016 "-" "KapacitorClient" 9dd28f4d-c43b-11e7-83e0-000000000000 37204
[httpd] 127.0.0.1 - - [08/Nov/2017:04:16:38 +0000] "GET /kapacitor/v1/tasks?dot-view=attributes&fields=link&limit=100&offset=0&pattern=load&replay-id=&script-format=formatted HTTP/1.1" 200 119 "-" "KapacitorClient" 9dd8d11e-c43b-11e7-83e1-000000000000 536
[httpd] 127.0.0.1 - - [08/Nov/2017:04:16:38 +0000] "PATCH /kapacitor/v1/tasks/load HTTP/1.1" 200 1024 "-" "KapacitorClient" 9dd8f40b-c43b-11e7-83e2-000000000000 25279
[httpd] 127.0.0.1 - - [08/Nov/2017:04:16:38 +0000] "GET /kapacitor/v1/tasks?dot-view=attributes&fields=link&limit=100&offset=0&pattern=load&replay-id=&script-format=formatted HTTP/1.1" 200 119 "-" "KapacitorClient" 9ddce37b-c43b-11e7-83e3-000000000000 633
[task_master:main] 2017/11/08 04:16:38 D! Starting task: load
[task_master:main] 2017/11/08 04:16:38 I! Started task: load
[task_master:main] 2017/11/08 04:16:38 D! digraph load {
query1 -> state_count2;
state_count2 -> state_count3;
state_count3 -> state_count4;
state_count4 -> alert5;
}
[edge:load|query1->state_count2] 2017/11/08 04:16:38 D! closing c: 0 e: 0
[edge:load|batch->batch0] 2017/11/08 04:16:38 I! aborting c: 0 e: 0
[load:query1] 2017/11/08 04:16:38 E! batch run aborted
[edge:load|state_count2->state_count3] 2017/11/08 04:16:38 D! closing c: 0 e: 0
[edge:load|state_count3->state_count4] 2017/11/08 04:16:38 D! closing c: 0 e: 0
[edge:load|state_count4->alert5] 2017/11/08 04:16:38 D! closing c: 0 e: 0

kapacitor delete tasks load
  634  kapacitor list tasks
  635  kapacitor define load -type batch -tick /etc/kapacitor/ticks/batch/load.tick  -dbrp telegraf.autogen
  636  kapacitor list tasks
  637  kapacitor enable load
  638  kapacitor list tasks
  639  kapacitor disabled load
enabling task diskio: batch query is not allowed to request data from "telegraf"."autogen"

[httpd] 127.0.0.1 - - [06/Nov/2017:03:49:28 +0000] "GET /kapacitor/v1/tasks?dot-view=attributes&fields=link&limit=100&offset=0&pattern=load&replay-id=&script-format=formatted HTTP/1.1" 200 119 "-" "KapacitorClient" 7d289288-c2a5-11e7-978a-000000000000 666
[httpd] 127.0.0.1 - - [06/Nov/2017:03:49:28 +0000] "DELETE /kapacitor/v1/tasks/load HTTP/1.1" 204 0 "-" "KapacitorClient" 7d28b91e-c2a5-11e7-978b-000000000000 3384

[httpd] 127.0.0.1 - - [06/Nov/2017:03:51:18 +0000] "GET /kapacitor/v1/tasks/load?dot-view=attributes&replay-id=&script-format=formatted HTTP/1.1" 404 57 "-" "KapacitorClient" bf0c15a5-c2a5-11e7-9ae6-000000000000 473
[httpd] 127.0.0.1 - - [06/Nov/2017:03:51:18 +0000] "POST /kapacitor/v1/tasks HTTP/1.1" 200 1279 "-" "KapacitorClient" bf0c3c49-c2a5-11e7-9ae7-000000000000 20389

[httpd] 127.0.0.1 - - [06/Nov/2017:03:51:53 +0000] "GET /kapacitor/v1/tasks?dot-view=attributes&fields=link&limit=100&offset=0&pattern=load&replay-id=&script-format=formatted HTTP/1.1" 200 119 "-" "KapacitorClient" d3c2ad78-c2a5-11e7-9c1a-000000000000 679
[task_master:main] 2017/11/06 03:51:53 I! Started task: load
[httpd] 127.0.0.1 - - [06/Nov/2017:03:51:53 +0000] "PATCH /kapacitor/v1/tasks/load HTTP/1.1" 200 1544 "-" "KapacitorClient" d3c2dc73-c2a5-11e7-9c1b-000000000000 31315


[task_master:main] 2017/11/06 06:36:27 I! Started task: diskio
[edge:diskio|batch->batch0] 2017/11/06 06:36:27 I! aborting c: 0 e: 0
[diskio:query1] 2017/11/06 06:36:27 E! batch run aborted




[httpd] 127.0.0.1 - - [06/Nov/2017:03:32:52 +0000] "GET /kapacitor/v1/tasks?dot-view=attributes&fields=link&limit=100&offset=0&pattern=load&replay-id=&script-format=formatted HTTP/1.1" 200 119 "-" "KapacitorClient" 2b8ddba7-c2a3-11e7-905c-000000000000 1106
[httpd] 127.0.0.1 - - [06/Nov/2017:03:32:52 +0000] "DELETE /kapacitor/v1/tasks/load HTTP/1.1" 204 0 "-" "KapacitorClient" 2b8e13a1-c2a3-11e7-905d-000000000000 3313

[httpd] 127.0.0.1 - - [06/Nov/2017:03:32:52 +0000] "GET /kapacitor/v1/tasks/load?dot-view=attributes&replay-id=&script-format=formatted HTTP/1.1" 404 57 "-" "KapacitorClient" 2b8f0c72-c2a3-11e7-905e-000000000000 12116
[httpd] 127.0.0.1 - - [06/Nov/2017:03:32:52 +0000] "POST /kapacitor/v1/tasks HTTP/1.1" 200 1280 "-" "KapacitorClient" 2b90fa30-c2a3-11e7-905f-000000000000 30341
[httpd] 127.0.0.1 - - [06/Nov/2017:03:32:52 +0000] "GET /kapacitor/v1/tasks?dot-view=attributes&fields=link&limit=100&offset=0&pattern=load&replay-id=&script-format=formatted HTTP/1.1" 200 119 "-" "KapacitorClient" 2b96521d-c2a3-11e7-9060-000000000000 560
[task_master:main] 2017/11/06 03:32:52 I! Started task: load
[edge:load|batch->batch0] 2017/11/06 03:32:52 I! aborting c: 0 e: 0
[load:query1] 2017/11/06 03:32:52 E! batch run aborted
[httpd] 172.31.3.11 - - [06/Nov/2017:03:32:57 +0000] "POST /write?consistency=&db=telegraf&precision=ns&rp=autogen HTTP/1.1" 204 0 "-" "InfluxDBClient" 2e74896a-c2a3-11e7-9062-000000000000 554
[httpd] 172.31.3.11 - - [06/Nov/2017:03:32:57 +0000] "POST /write?consistency=&db=telegraf&precision=ns&rp=autogen HTTP/1.1" 204 0 "-" "InfluxDBClient" 2e74d955-c2a3-11e7-9063-000000000000 513
[httpd] 172.31.3.11 - - [06/Nov/2017:03:32:57 +0000] "POST /write?consistency=&db=telegraf&precision=ns&rp=autogen HTTP/1.1" 204 0 "-" "InfluxDBClient" 2e74f824-c2a3-11e7-9064-000000000000 480
[httpd] 172.31.3.11 - - [06/Nov/2017:03:32:57 +0000] "POST /write?consistency=&db=telegraf&precision=ns&rp=autogen HTTP/1.1" 204 0 "-" "InfluxDBClient" 2e751575-c2a3-11e7-9065-000000000000 529
[httpd] 172.31.3.11 - - [06/Nov/2017:03:32:57 +0000] "POST /write?consistency=&db=telegraf&precision=ns&rp=autogen HTTP/1.1" 204 0 "-" "InfluxDBClient" 2e7533ce-c2a3-11e7-9066-000000000000 491
[httpd] 172.31.3.11 - - [06/Nov/2017:03:32:57 +0000] "POST /write?consistency=&db=telegraf&precision=ns&rp=autogen HTTP/1.1" 204 0 "-" "InfluxDBClient" 2e754fef-c2a3-11e7-9067-000000000000 499
[httpd] 172.31.3.11 - - [06/Nov/2017:03:33:00 +0000] "POST /write?consistency=&db=_internal&precision=ns&rp=monitor HTTP/1.1" 204 0 "-" "InfluxDBClient" 3012fd53-c2a3-11e7-9068-000000000000 4321
[httpd] 172.31.3.11 - - [06/Nov/2017:03:33:00 +0000] "POST /write?consistency=&db=_internal&precision=ns&rp=monitor HTTP/1.1" 204 0 "-" "InfluxDBClient" 301580fc-c2a3-11e7-9069-000000000000 2943
[httpd] 172.31.3.11 - - [06/Nov/2017:03:33:00 +0000] "POST /write?consistency=&db=_internal&precision=ns&rp=monitor HTTP/1.1" 204 0 "-" "InfluxDBClient" 30164641-c2a3-11e7-906a-000000000000 4005

--------------------------------
var load_data = batch
    |query('''
        SELECT * FROM "telegraf"."autogen"."system"
    ''')
        .period(60s)
        .every(10s)
        .groupBy('host')
    |stateCount(lambda: "load5" > 2)


load_data
    |alert()

        .stateChangesOnly(4m)

        .crit(lambda: "state_count" >= 4)

        .critReset(lambda: "state_count" <= 0)

        .message('{{ if eq .Level "OK" }}RECOVERY{{ else }}PROBLEM{{ end }}: LOAD on server {{ index .Fields "ip"}} is {{ .Level }}')

        .details('''
<p style="font-family:courier;color:white;{{ if eq .Level "CRITICAL" }}background-color:red{{ else if eq .Level "WARNING" }}background-color:orange{{ else if eq .Level "INFO" }}background-color:blue{{ else }}background-color:green{{ end }};text-align:center">Monitoring System Notification</p>

<p style="font-family:courier;">

<b>Server name:</b> {{ index .Tags "host"}}</br>

<b>Server IP:</b> {{ index .Fields "ip" }}</br>

<b>Service:</b> Check system load</br>

<b>Level:</b> {{ if eq .Level "CRITICAL" }}<font color="red">{{ else if eq .Level "WARNING" }}<font color="orange">{{ else if eq .Level "INFO" }}<font color="blue">{{ else }}<font color="green">{{ end }}{{ .Level }}</font></br>

<b>System load:</b> {{ index .Fields "load1" }}   {{ index .Fields "load5" }}   {{ index .Fields "load15" }}</br>

<b>Time:</b> {{ .Time }}</br>
</p>
''')
        .email()




